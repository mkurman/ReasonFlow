{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from reasonflow import ReasonFlowConfig, ReasonFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca709a96e8d740618809bded1c3a408b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'mkurman/llama-3.2-MEDIT-3B-o1'\n",
    "hf_cache = os.environ.get('HF_HOME', os.path.expanduser('~/hf_cache'))\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    cache_dir=hf_cache,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    path,\n",
    "    cache_dir=hf_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model.config.eos_token_id, int):\n",
    "    model.config.eos_token_id = [model.config.eos_token_id]\n",
    "\n",
    "def has_eos(output, eos_token: list[int]):\n",
    "    return any(x for x in eos_token if x in output)    \n",
    "\n",
    "def dtype_to_str(dtype):\n",
    "    \"\"\"Convert torch.dtype to string without 'torch.' prefix.\"\"\"\n",
    "    return str(dtype).split('.')[-1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_like_texts = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"What is the strength (B1) of a 13C 90° pulse of duration 1 μs?\",\n",
    "        },\n",
    "    ],\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": 'What is the maximum value of the z-component of the spin angular momentum of 43Ca? [I = 7⁄2] [ \"10.492 x 10^-34 J s\", \"2.549 x 10^-34 J s\", \"5.493 x 10^-34 J s\", \"4.185 x 10^-34 J s\", \"1.238 x 10^-34 J s\", \"8.370 x 10^-34 J s\", \"6.234 x 10^-34 J s\", \"9.854 x 10^-34 J s\", \"3.691 x 10^-34 J s\", \"7.123 x 10^-34 J s\" ]',\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "input_ids = tokenizer(tokenizer.apply_chat_template(chat_like_texts[1], tokenize=False, add_generation_prompt=True), padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_THOUGHTS = 4 # Maximum number of thoughts to validate\n",
    "NUM_OF_THINKERS = 6 # Maximum number of thinkers to reason with\n",
    "TOPK_THINKERS = 2 # Maximum number of thinkers to consider\n",
    "ACCEPTANCE_THRESHOLD = 0.5 # Decision threshold for accepting a token with lower probability than the most probable token\n",
    "TEMPERATURES = [0.7, 1.3] # Temperature values for sampling less probable tokens\n",
    "\n",
    "MAX_TOTAL_TOKENS = 8192 # Maximum number of tokens to generate in total\n",
    "USE_TOKENS = True # Whether to use tokens for sampling or not (using tokens for stability)\n",
    "\n",
    "DYNAMIC_TEMPERATURE = True # Whether to use dynamic temperature or not\n",
    "DIVERSITY_WEIGHT = 0.3 # Weight for diversity in sampling\n",
    "UNCERTAINTY_THRESHOLD = 0.5 # Appropriate threshold for uncertainty in sampling\n",
    "MIN_ACCEPTANCE_THRESHOLD = 0.3 # Minimum acceptance threshold for sampling\n",
    "MAX_ACCEPTANCE_THRESHOLD = 0.7 # Maximum acceptance threshold for sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the ReasonFlow algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook registered for model.norm\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 20 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the maximum value of the z-component of the spin angular momentum of 43Ca? [I = 7⁄2] [ \"10.492 x 10^-34 J s\", \"2.549 x 10^-34 J s\", \"5.493 x 10^-34 J s\", \"4.185 x 10^-34 J s\", \"1.238 x 10^-34 J s\", \"8.370 x 10^-34 J s\", \"6.234 x 10^-34 J s\", \"9.854 x 10^-34 J s\", \"3.691 x 10^-34 J s\", \"7.123 x 10^-34 J s\" ]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Thought>\n",
      "Alright, I need to determine the maximum value of the z-component of the spin angular momentum of 43Ca, given the options provided. Let's break this down step by step to ensure accuracy.\n",
      "\n",
      "First, I recall that the spin angular momentum (J) of an atom is related to its nuclear spin (I) by the equation:\n",
      "\n",
      "\\[ J = \\sqrt{I(I + 1)} \\hbar \\]\n",
      "\n",
      "where \\( \\hbar \\) is the reduced Planck constant. For calcium-43 (Ca), the nuclear spin (I) is given as \\( \\frac{7}{2} \\).\n",
      "\n",
      "So, plugging in the values:\n",
      "\n",
      "\\[ J = \\sqrt{\\frac{7}{2} \\left(\\frac{7}{2} + 1\\right)} \\hbar \\]\n",
      "\\[ J = \\sqrt{\\frac{7}{2} \\times \\frac{9}{2}} \\hbar \\]\n",
      "\\[ J = \\sqrt{\\frac{63}{4}} \\hbar \\]\n",
      "\\[ J = \\frac{\\sqrt{63}}{2} \\hbar \\]\n",
      "\\[ J = \\frac{7.937}{2} \\hbar \\]\n",
      "\\[ J \\approx 3.9685 \\hbar \\]\n",
      "\n",
      "Now, the z-component of the spin angular momentum (Jz) is given by:\n",
      "\n",
      "\\[ J_z = m_J \\hbar \\]\n",
      "\n",
      "where \\( m_J \\) is the magnetic quantum number, which ranges from -I to +I. For \\( I = \\frac{7}{2} \\), the possible values of \\( m_J \\) are:\n",
      "\n",
      "- \\( m_J = -\\frac{7}{2} \\)\n",
      "- \\( m_J = -\\frac{5}{2} \\)\n",
      "- \\( m_J = -\\frac{3}{2} \\)\n",
      "- \\( m_J = -\\frac{1}{2} \\)\n",
      "- \\( m_J = \\frac{1}{2} \\)\n",
      "- \\( m_J = \\frac{3}{2} \\)\n",
      "- \\( m_J = \\frac{5}{2} \\)\n",
      "- \\( m_J = \\frac{7}{2} \\)\n",
      "\n",
      "The maximum value of \\( J_z \\) occurs when \\( m_J = \\frac{7}{2} \\):\n",
      "\n",
      "\\[ J_z = \\frac{7}{2} \\hbar \\]\n",
      "\n",
      "Now, let's convert this to the given units. The reduced Planck constant \\( \\hbar \\) is approximately \\( 1.0545718 \\times 10^{-34} \\) J s.\n",
      "\n",
      "\\[ J_z = \\frac{7}{2} \\times 1.0545718 \\times 10^{-34} \\]\n",
      "\\[ J_z \\approx 3.687 \\times 10^{-34} \\text{ J s} \\]\n",
      "\n",
      "Comparing this to the provided options:\n",
      "\n",
      "- 10.492 x 10^-34 J s\n",
      "- 2.549 x 10^-34 J s\n",
      "- 5.493 x 10^-34 J s\n",
      "- 4.185 x 10^-34 J s\n",
      "- 1.238 x 10^-34 J s\n",
      "- 8.370 x 10^-34 J s\n",
      "- 6.234 x 10^-34 J s\n",
      "- 9.854 x 10^-34 J s\n",
      "- 3.691 x 10^-34 J s\n",
      "- 7.123 x 10^-34 J s\n",
      "\n",
      "The calculated value of \\( J_z \\) is approximately 3.687 x 10^-34 J s, which is closest to 3.691 x 10^-34 J s among the given options. Therefore, the maximum value of the z-component of the spin angular momentum of 43Ca is 3.691 x 10^-34 J s.\n",
      "\n",
      "</Thought>\n",
      "<Output>\n",
      "The maximum value of the z-component of the spin angular momentum of 43Ca is 3.691 x 10^-34 J s.\n",
      "</Output><|eot_id|>"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "input_tokens = input_ids['input_ids'].to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "config = ReasonFlowConfig(\n",
    "    num_of_thinkers=NUM_OF_THINKERS,\n",
    "    num_of_thoughts=NUM_OF_THOUGHTS,\n",
    "    topk_thinkers=TOPK_THINKERS,\n",
    "    acceptance_threshold=ACCEPTANCE_THRESHOLD,\n",
    "    temperatures=TEMPERATURES,\n",
    "    dynamic_temperature=DYNAMIC_TEMPERATURE,\n",
    "    diversity_weight=DIVERSITY_WEIGHT,\n",
    "    uncertainty_threshold=UNCERTAINTY_THRESHOLD,\n",
    "    min_acceptance_threshold=MIN_ACCEPTANCE_THRESHOLD,\n",
    "    max_acceptance_threshold=MAX_ACCEPTANCE_THRESHOLD,\n",
    ")\n",
    "\n",
    "index = 1\n",
    "\n",
    "reason_flow = ReasonFlow(config, model, tokenizer)\n",
    "output = reason_flow.generate_thoughts(chat_like_texts[index], \n",
    "                                       use_tokens=USE_TOKENS,\n",
    "                                       max_new_tokens=4096, \n",
    "                                       stream=True,\n",
    "                                       device=device, \n",
    "                                       torch_dtype=dtype_to_str(model.dtype),\n",
    "                                       do_sample=False, \n",
    "                                       temperature=None, \n",
    "                                       top_p=None, \n",
    "                                       top_k=None, \n",
    "                                       repetition_penalty=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
